User will give you text of the CSV he obtained from provider of e-resources usage statistics.
As the CSVs he obtains from different providers are different, he has written universal python parser,  which can parse the data out. For it to work, he needs you to create the parser rules in the correct format.
Special instructions from the user - keep them in mind:
You know from the user that metrics in this file are: {{ metrics }}.
That means that you should try to locale cells with these metrics in them. 
The date should span months from {{ month_first }} to {{ month_last }}.

{% if dimensions %}

The user also told you that the dimensions in this file are: {{ dimensions }}.
That means that you should try to locate cells with these dimensions in them.
{% else %}
There are no dimensions in this file.
{% endif %}

{% if title_report %}
The user also told you that this is a title report, which means you should include information about title and title identifiers.
The title identifiers are {{ title_identifiers }}.
{% else %}
The user also told you that this is not a title report, which means you should not include information about title and title identifiers.
{% endif %}

{% if user_comment  %}
This is a user comment to consider: {{ user_comment }}
{% endif %}

You also know this data is from platform {{ platform_name }}.
Comment all your thinking out loud.
First analyze headers, distinguish which sources belong to headers and which ones do not.
Keep in mind that the coordinates are global, zero-based.
Then analyze where can you find dates, metrics, dimensions and titles in the file.
Analyze whether the dates are composed or not.

After you finish creating the parsing rules, give them to check_parsing_rules function as string, this function will validate the format, and try to parse the data out.
It may return either validation errors, or some parsing errrors, which should indicate what is wrong with the parsing rules.
If it returns validation errors, you should fix them and try again, until it passes.
When it passes it will reuturn True, and you can stop and return the parsing rules.
Do not stop until you get True from the check_parsing_rules function.

This is documentation regarding the parser format:
#Parsing Rules

##Coordinate Representation
To navigate the files and locate relevant pieces of information, a Source model is used.

Source = typing.Union[Coord, CoordRange, Value, SheetAttr]

### Coordinate System
- Indexing is zero-based (Row 0 is the first row, Column 0 is Column A).

### Source Types
Each Source defines where a value comes from. It can be one of the following:
1. Coordinate ("coord"): A single specific cell
2. Coordinate Range: Starts at a specific cell and iterates in a direction to extract multiple values.
    - Required: "coord" (start), "direction" ("left", "right", "up" or "down")
    - Optional: "max_count" - limit the number of items
    Example: {"coord": { "row": 2, "col": 1 }, "direction": "right", "max_count": 3}
3. Constant value ("value"): A static value hardcoded in the JSON, useful for filling in missing fixed data usually coming from the user note.
4. SheetAttribute ("sheet_attr"): Extracts metadata from the spreadsheet properties rather than cell contents.
    - Use this when data (like Year or Organization) is in the Excel Tab Name instead of a cell.
    - Literal["name", "sheet_idx"]

##Extraction Parameters
The Extraction Parameters model provides additional constraints and methods for retrieving data from a given Source. 
These include:
\begin{itemize}
    \item \textbf{default} - Specifies a fallback value to be used when an empty cell is encountered.
    \item \textbf{last\_value\_as\_default} - A boolean flag. If true, when a "blank" cell is encountered, the parser will automatically reuse the value from the previous cell. This is essential for merged headers (e.g., "January" spanning two columns where the second cell is physically empty).
    \item \textbf{regex} - Defines a regular expression pattern to extract specific information from cell content.
    \item \textbf{blank_values} - It explicitly defines what is considered "blank" for the purpose of the last_value_as_default: true logic, by default (None, "").
    \item \textbf{on\_validation\_error} - Determines the action taken when extraction or validation fails (e.g., regex mismatch, wrong date format, or empty cell). This parameter can have one of the following values:
    \begin{itemize}
        \item \textbf{fail} - The validation failure is raised internally. This allows the parser to catch the error and apply the default value (or last value). Use this when you expect empty cells that should be filled in.
        \item \textbf{skip} - The entire column/row is ignored and discarded from processing.
        \item \textbf{stop} - The parser aborts processing immediately.
        \item \textbf{pass} - The invalid value is accepted without error handling.
    \end{itemize}
\end{itemize}

##Content Extractors
The Source and Extraction Parameters are utilized within Content Extractor models, 
each of which specialized in extracting a specific type of data from the file.
Each Extractor model is defined by:
\begin{itemize}
    \item \textbf{Source} indicating the data location.
    \item \textbf{Extraction Parameters}, which define how the data should be retrieved.
    \item \textbf{Role}, specifying the type of information extracted (e.g. dates, metrics).
\end{itemize}

Metrics are extracted using the \textbf{MetricSource} specialized Content Extractor model.
The goal of this extractor is to extract where in the file are the metric names located.
It has the \textbf{source}, \textbf{extract parameters}, and \textbf{role} parameters, with role set to 'metric'.
Organizations are extracted using the \textbf{OrganizationSource} specialized Content Extractor model.
The goal of this extractor is to extract where in the file are the organization names located.
The role is set to 'organization'.
Dimensions has \textbf{source}, \textbf{extract parameters}, and \textbf{role} parameters, with role set to 'dimension'.
It also introduces \textbf{name} parameter, which specifies the extracted dimension.
The goal of the dimensions is to find the values of the dimension of the given name in the file.
Titles are extracted using the \textbf{TitleSource} specialized Content Extractor model.
This model does not introduce any additional parameters beyond the \textbf{source}, \textbf{extract parameters}, and \textbf{role}, which is set to 'title'.

##Dates

Dates are extracted using a specialized Content Extractor model \textbf{DateSource}. 
Like other content extractors, this model is defined by three key components: Source, Extraction Parameters, and a predefined Role, which in this case is set to 'date'.


### Handling split dates
In some cases, month and year information is located in separate cells. How you configure this depends on whether the date parts act as headers.

There are two cases when this can happen:

1. The month and year are both either in headers or outside the headers (in the same axis)
In this case, this can be parsed using the \textbf{composed} parameter, which allows to specify separate sources for the year and month values. 

2. One Part is a Header, One Part is a Row Label (Matrix Table)
A common scenario in reports is where Months are Column Headers (Top) and Years are Row Headers (Left side), or vice versa.
In this case, you must define TWO separate date sources and use data_cells_options to merge them.

- Define the Row Label date in the main dates section (e.g., the Year in Column A).
- Define the Column Header date inside data_headers.roles (e.g., the Month in Row 1).
- CRITICAL: Set data_cells_options inside data_headers to tell the parser which part comes from where.

Mechanism (data_cells_options):
use_header_year: If false, the year comes from the Row Label (dates). If true, it comes from the Header (data_headers).
use_header_month: If false, the month comes from the Row Label (dates). If true, it comes from the Header (data_headers).

This extractor also has optional \textbf{date\_pattern} parameter, which specifies the format of the date in the source, 
which helps parsing non-standard date formats. This should be provided as strftime pattern.

Example: Matrix Table (Year in Row, Month in Header)
    Scenario: Column A contains "2024", "2025". Row 0 contains "Jan", "Feb".
    Setup:
    Main dates: Source points to Col A (%Y).
    Header role: Source points to Row 0 (%b).
    data_cells_options: {"use_header_year": false, "use_header_month": true}.

This is the DateSource pydantic model:
class DateSource(BaseModel):
    source: Source
    extract_params: ExtractParams | None = ExtractParams()
    composed: ComposedDate | None = None
    date_pattern: str | None = None - should be strftime pattern, such as %b-%y
    role: typing.Literal[RoleEnum.DATE]

and this is the ComposedDate:

class ComposedDate(BaseModel):
    year: "DateSource"
    month: "DateSource" 

This is an example of composed date, with month and year being on different rows:
"composed": {
    "year": {
        "source": {
            "coord": {"row": 0,"col": 5},
            "direction": "right"},
        "role": "date"},
    "month": {
        "source":
            {"coord": {"row": 1,"col": 5},
            "direction": "right"},
        "role": "date"}
}


##Title Identifiers

Title identifiers, such as ISSN, ISBN, and DOI, are handled by the \textbf{TitleIdSource} Content Extractor model.
This model includes a \textbf{name} parameter, which specifies the type of title identifier extracted.
Supported options are ISBN, Print ISSN, Online ISSN, DOI and URI.
Any identifier that does not fit into one of these categories is considered Proprietary.
The role of this extractor is set to 'title\_id'.


##Data Headers
Data Headers model serves for defining the headers of the data.
Headers may include different data, from dates and metrics to dimensions.
They are again distinguished by the role parameter.

This is the pydantic model:
class DataHeaders(BaseModel):
    roles: typing.List[typing.Union[MetricSource, DateSource]]
    data_cells: CoordRange
    data_direction: Direction
    data_cells_options: DataCellsOptions | None = None
    data_extract_params: ExtractParams | None = None

This model is also used to locate the numerical values of the data.
The \textbf{data\_cells} parameter specifies the location of the first data row/column, which are the actual values in the table.
The \textbf{data\direction} parameter specifies the "flow" of the values for a single column/series:
- "down": Use this for standard tables where headers are at the top and values flow vertically down (e.g., row 4, row 5, row 6). This is perpendicular to the headers.
- "right": Use this for transposed tables where headers are on the left and values flow horizontally to the right.
The "data_cells_options" specifies how to metge dates that are not in the same axis (see Handling Split Dates)
The "data_extract_params" is again the Extraction Parameters. This is mostly used using {"default" : 0} when there are some numerical data cells that are empty/nulls, to fill the data in.

This is an example of Data Headers model, with date in the headers and the data cells.
The first data cells is in (1,1), the one data is in one row, and data direction is down.

{"roles" : [{
    "source": {"coord": 
        {"row": 0,"col": 1},
        "direction": "right"},
        "role": "date" }],
"data_direction": "down",
"data_cells": {
    "coord": {
        "row": 1,"col": 1},
        "direction": "right"}
}



##Area Definition
Combining all the parsing rules together is Area model.

The parser works by iterating through the data cells defined in data_headers. For every column (or row) it processes, it needs to know what that column represents.
If the definition changes with every column (e.g., Column A is "Logins", Column B is "Searches"):
- You MUST define this inside data_headers.roles.
- If you leave data_headers.roles empty, the parser will iterate the columns but have no idea what they are, resulting in empty output.
If the definition is static or found in the row itself (e.g., Column A is "Title", Column B is "Metric Name"):
- You define this in the top-level fields (metrics, titles, dates) outside of data_headers.

Example scenarios:
> Table with Months across the top: The date role must be inside data_headers.roles.
> Table with Metrics across the top: The metric role must be inside data_headers.roles.
> Table with Years down the side: The date source is defined in the top-level dates field.

Each parser definition file can include multiple areas (multiple tables in one file / sheet).

Each area is defined by:
\begin{itemize}
    \item \textbf{data headers} - DataHeaders model
    \item \textbf{metrics} - optional MetricSource model, used when the metrics are not in the headers
    \item \textbf{dates} - optional DateSource model, used when the dates are not in the headers
    \item \textbf{titles} - optional TitleSource model, used when the titles are not in the headers
    \item \textbf{title ids} - optional list of TitleIdSource models
    \item \textbf{dimensions} - optional list of DimensionSource models, used when the dimensions are not in the headers
    \item \textbf{organizations} - optional list of OrganizationSource models
    \item \textbf{kind} - should always be set to "non_counter.generic"
\end{itemize}

this is the pydantic model:
class NonCounterGeneric(BaseModel): #Generic Area Definition
    metrics: typing.Optional[MetricSource]
    data_headers: DataHeaders
    dates: typing.Optional[DateSource]
    titles: typing.Optional[TitleSource]
    title_ids: typing.List[TitleIdSource] = []
    dimensions: typing.List[DimensionSource] = []
    organizations: typing.Optional[OrganizationSource]
    kind: typing.Literal["non_counter.generic"] = "non_counter.generic"

Each information can be localized in either Data Headers (using appropriate roles) or outside the data headers
using the respective fields.
This can be even combined, when for example one part of date is in the headers and the other part is not.

\section{Heuristics}
Second part of each Parser Definition File is Heuristics. For now, keep this empty and return {}.

\section{Metadata}
Metadata include the \textbf{parser\_name}, \textbf{platforms} that the parser can be used for and additional
information for the data processing. This additional information is not neccessarily obtained from the files,
is often obtanied from either historical knowledge or from the customer. You will find it in the user comment if its relevant.
Sometimes multiple platforms share the same or very similar format, so we can use the same parser for multiple platforms.

The \textbf{metric\_aliases} is used when we have previous information that one metric can have several different names 
for this particular platform. It mainly serves to prevent small mismatches such as View/Views/View*.
It defines one resulting name to which all the other names should be mapped.
The \textbf{dimension\_aliases} function in the same manner for information about dimensions.

The \textbf{metrics\_to\_skip} parameter is used when we have previous information that the customer does not want this metric.
It is also used in cases when metrics and dimensions are mixed in the same column or row and we want to skip between them.
The \textbf{titles\_to\_skip} and \textbf{dimensions\_to\_skip} work in the same manner for titles and dimensions.

available metrics - list of all metrics

To summarize, the metadata section contains the following fields:
\begin{itemize}[parsep=0pt, itemsep=0pt, leftmargin=0.5cm]
    \item \textbf{parser name} - The name of the parser, used for identification, usually consists of the platform name.
    \item \textbf{data format} - The format of the data, used for identification, please always fill in "data_format": {"name":"simple_format"}.
    \item \textbf{kind} - The kind of the parser, used for identification, please always fill in "non_counter.generic".
    \item \textbf{heuristics} - please always fill in "heuristics": {"conds": [], "kind": "and"}
    \item \textbf{platforms} - A list of platforms which can be processed by this parser, user will tell you the platform.
    \item \textbf{metrics to skip} - A list of metrics that should be skipped during processing.
    \item \textbf{available metrics} - A list of metrics that are available for processing.
    \item \textbf{titles to skip} - A list of titles that should be skipped during processing.
    \item \textbf{dimensions to skip} - A list of dimensions that should be skipped during processing.
    \item \textbf{metric aliases} - A dictionary of metric aliases.
    \item \textbf{dimension aliases} - A dictionary of dimension aliases.
\end{itemize}



This is the final structure:
class ParserDefinition(BaseModel):
    parser_name: str
    data_format: DataFormat
    kind: typing.Literal["non_counter.generic"] = "non_counter.generic"
    heuristics: Heuristics 
    areas: typing.List[NonCounterGeneric]
    platforms: typing.List[str] = []
    metrics_to_skip: typing.List[str] = []
    available_metrics: typing.Optional[typing.List[str]]
    on_metric_check_failed: TableException.Action = TableException.Action.skip
    titles_to_skip: typing.List[str] = []
    dimensions_to_skip: typing.Dict[str, typing.List[str]] = {}
    metric_aliases: typing.List[typing.Tuple[str, str]] = []
    dimension_aliases: typing.List[typing.Tuple[str, str]] = []


This it the area structure:
class NonCounterGeneric(BaseModel): #Generic Area Definition
    metrics: typing.Optional[MetricSource]
    data_headers: DataHeaders
    dates: typing.Optional[DateSource]
    titles: typing.Optional[TitleSource]
    title_ids: typing.List[TitleIdSource] = []
    dimensions: typing.List[DimensionSource] = []
    organizations: typing.Optional[OrganizationSource]