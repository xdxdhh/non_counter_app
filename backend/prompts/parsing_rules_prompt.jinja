User will give you text of the CSV he obtained from provider of e-resources usage statistics.
As the CSVs he obtains from different providers are different, he has written universal python parser,  which can parse the data out. For it to work, he needs you to create the parser rules in the correct format.
Special instructions from the user - keep them in mind:
You know from the user that metrics in this file are: {{ metrics }}.
That means that you should try to locale cells with these metrics in them. 
The date should span months from {{ month_first }} to {{ month_last }}.

{% if dimensions %}

The user also told you that the dimensions in this file are: {{ dimensions }}.
That means that you should try to locate cells with these dimensions in them.
{% else %}
There are no dimensions in this file.
{% endif %}

{% if title_report %}
The user also told you that this is a title report, which means you should include information about title and title identifiers.
The title identifiers are {{ title_identifiers }}.
{% else %}
The user also told you that this is not a title report, which means you should not include information about title and title identifiers.
{% endif %}

You also know this data is from platform {{ platform_name }}.
Comment all your thinking out loud.
First analyze headers, distinguish which sources belong to headers and which ones do not.
Keep in mind that the coordinates are global, zero-based.
Then analyze where can you find dates, metrics, dimensions and titles in the file.
Analyze whether the dates are composed or not.

After you finish creating the parsing rules, give them to check_parsing_rules function as string, this function will validate the format, and try to parse the data out.
It may return either validation errors, or some parsing errrors, which should indicate what is wrong with the parsing rules.
If it returns validation errors, you should fix them and try again, until it passes.
When it passes it will reuturn True, and you can stop and return the parsing rules.
Do not stop until you get True from the check_parsing_rules function.

This is documentation regarding the parser format:
##Parsing Rules

###Coordinate Representation
To navigate the files and locate relevant pieces of information, Source model is used.
All the coordinates are global, with first cell having (0,0) coordinate.
Coordinates are defined using zero-based indexing, where each coordinate represents a specific row and column in the input file. The first cell in the input file has  (0,0) coordinate.
A Coordinate Range extends this definition by specifying an initial cell coordinate and a parsing direction (left, right, up, down).
A Coordinate Range also includes an optional maximum count parameter, which limits the number of row/columns extracted in the specified direction.

Example - this starts at cell (2, 1) and goes right, extracting up to 3 values:

{"coord": { "row": 2, "col": 1 },
"direction": "right",
"max_count": 3}


Each \textbf{Source} of a value within the file is as one of the following:
\begin{itemize}
    \item \textbf{Coordinate} - A single cell in the file.
    \item \textbf{Coordinate Range} - Starting cell and direction for extracting multiple values.
    \item \textbf{Constant value} - A predefined static value, independent of the file content.
\end{itemize}

The source determines the location of specific data within the file, which is then repeatedly used in the parsing rules.

##Extraction Parameters
The \textbf{Extraction Parameters} model provides additional constraints and methods for retrieving data from a given Source. 
These include:
\begin{itemize}
    \item \textbf{default} - Specifies a fallback value to be used when an empty cell is encountered.
    \item \textbf{last\_value\_as\_default} - Uses the last successfully extracted value as the default for empty cells, useful when all values are expected to be identical.
    \item \textbf{regex} - Defines a regular expression pattern to extract specific information from cell content.
    \item \textbf{on\_validation\_error} - Determines the action taken when extraction fails. This parameter can have one of the following values:
    \begin{itemize}
        \item \textbf{fail} - The process stops and reports an error.
        \item \textbf{skip} - The erroneous value is skipped, and processing continues.
        \item \textbf{stop} - The process stops and reports an error.
        \item \textbf{pass} - The invalid value is accepted without error handling.
    \end{itemize}
\end{itemize}

##Content Extractors
The Source and Extraction Parameters are utilized within Content Extractor models, 
each of which specialized in extracting a specific type of data from the file.
Each Extractor model is defined by:
\begin{itemize}
    \item \textbf{Source} indicating the data location.
    \item \textbf{Extraction Parameters}, which define how the data should be retrieved.
    \item \textbf{Role}, specifying the type of information extracted (e.g. dates, metrics).
\end{itemize}

Metrics are extracted using the \textbf{MetricSource} specialized Content Extractor model.
The goal of this extractor is to extract where in the file are the metric names located.
It has the \textbf{source}, \textbf{extract parameters}, and \textbf{role} parameters, with role set to 'metric'.
Organizations are extracted using the \textbf{OrganizationSource} specialized Content Extractor model.
The goal of this extractor is to extract where in the file are the organization names located.
The role is set to 'organization'.
Dimensions has \textbf{source}, \textbf{extract parameters}, and \textbf{role} parameters, with role set to 'dimension'.
It also introduces \textbf{name} parameter, which specifies the extracted dimension.
The goal of the dimensions is to find the values of the dimension of the given name in the file.
Titles are extracted using the \textbf{TitleSource} specialized Content Extractor model.
This model does not introduce any additional parameters beyond the \textbf{source}, \textbf{extract parameters}, and \textbf{role}, which is set to 'title'.

##Dates

Dates are extracted using a specialized Content Extractor model \textbf{DateSource}. 
Like other content extractors, this model is defined by three key components: Source, Extraction Parameters, and a predefined Role, which in this case is set to 'date'.
In some cases month and year information is located in separate fields in the file rather than being a single entity.

There are two cases when this can happen:
1. The month and year are both either in headers or outside the headers.
In this case, this can be parsed using the \textbf{composed} parameter, which allows to specify separate sources for the year and month values. 
An example of its usage is shown in Figure \ref{fig:composed_dates}.

2. One part of the date is in the headers and the other part is not.
In this case, the \textbf{composed} cannot be used, and instead, two separate date sources must be defined - one in the headers and one outside the headers.
Each of this date sources only specifies one part of the date, either month or year.

This extractor also has optional \textbf{date\_pattern} parameter, which specifies the format of the date in the source, 
which helps parsing non-standard date formats.


This is the DateSource pydantic model:
class DateSource(BaseModel):
    source: Source
    extract_params: ExtractParams | None = ExtractParams()
    composed: ComposedDate | None = None
    date_pattern: str | None = None
    role: typing.Literal[RoleEnum.DATE]

and this is the ComposedDate:

class ComposedDate(BaseModel):
    year: "DateSource"
    month: "DateSource" 

This is an example of composed date, with month and year being on different rows:
"composed": {
    "year": {
        "source": {
            "coord": {"row": 0,"col": 5},
            "direction": "right"},
        "role": "date"},
    "month": {
        "source":
            {"coord": {"row": 1,"col": 5},
            "direction": "right"},
        "role": "date"}
}


##Title Identifiers

Title identifiers, such as ISSN, ISBN, and DOI, are handled by the \textbf{TitleIdSource} Content Extractor model.
This model includes a \textbf{name} parameter, which specifies the type of title identifier extracted.
Supported options are ISBN, Print ISSN, Online ISSN, DOI and URI.
Any identifier that does not fit into one of these categories is considered Proprietary.
The role of this extractor is set to 'title\_id'.


##Data Headers
Data Headers model serves for defining the headers of the data.
Headers may include different data, from dates and metrics to dimensions.
They are again distinguished by the role parameter.

This model is also used to locate the numerical values of the data.
The \textbf{data\_cells} parameter specifies the location of the first data row/column, which are the actual values in the table.
The \textbf{data\_direction} parameter specifies the direction in which the data is organized,
indicating whether the next data will be obtained by going 'down' or 'right' from the data cells.

This is an example of Data Headers model, with date in the headers and the data cells.
The first data cells is in (1,1), the one data is in one row, and data direction is down.

{"roles" : [{
    "source": {"coord": 
        {"row": 0,"col": 1},
        "direction": "right"},
        "role": "date" }],
"data_direction": "down",
"data_cells": {
    "coord": {
        "row": 1,"col": 1},
        "direction": "right"}
}



##Area Definition
Combining all the parsing rules together is Area model.

In general (and this is very important), all the information can either be included in headers, or somewhere else in the file.
When included in the headers, its Content Extractor will be under 'headers'.
When included outside the headers, its Content Extractor will under its specialized field (e.g. metrics, dates, titles, etc.).

Each parser definition file can include multiple areas (multiple tables in one file / sheet).

Each area is defined by:
\begin{itemize}
    \item \textbf{data headers} - DataHeaders model
    \item \textbf{metrics} - optional MetricSource model, used when the metrics are not in the headers
    \item \textbf{dates} - optional DateSource model, used when the dates are not in the headers
    \item \textbf{titles} - optional TitleSource model, used when the titles are not in the headers
    \item \textbf{title ids} - optional list of TitleIdSource models
    \item \textbf{dimensions} - optional list of DimensionSource models, used when the dimensions are not in the headers
    \item \textbf{organizations} - optional list of OrganizationSource models
    \item \textbf{kind} - should always be set to "non_counter.generic"
\end{itemize}

this is the pydantic model:
class NonCounterGeneric(BaseModel): #Generic Area Definition
    metrics: typing.Optional[MetricSource]
    data_headers: DataHeaders
    dates: typing.Optional[DateSource]
    titles: typing.Optional[TitleSource]
    title_ids: typing.List[TitleIdSource] = []
    dimensions: typing.List[DimensionSource] = []
    organizations: typing.Optional[OrganizationSource]
    kind: typing.Literal["non_counter.generic"] = "non_counter.generic"

Each information can be localized in either Data Headers (using appropriate roles) or outside the data headers
using the respective fields.
This can be even combined, when for example one part of date is in the headers and the other part is not.

\section{Heuristics}
Second part of each Parser Definition File is Heuristics. For now, keep this empty and return {}.

\section{Metadata}
Metadata include the \textbf{parser\_name}, \textbf{platforms} that the parser can be used for and additional
information for the data processing. This additional information is not neccessarily obtained from the files,
is often obtanied from either historical knowledge or from the customer. You will find it in the user comment if its relevant.
Sometimes multiple platforms share the same or very similar format, so we can use the same parser for multiple platforms.

The \textbf{metric\_aliases} is used when we have previous information that one metric can have several different names 
for this particular platform. It mainly serves to prevent small mismatches such as View/Views/View*.
It defines one resulting name to which all the other names should be mapped.
The \textbf{dimension\_aliases} function in the same manner for information about dimensions.

The \textbf{metrics\_to\_skip} parameter is used when we have previous information that the customer does not want this metric.
It is also used in cases when metrics and dimensions are mixed in the same column or row and we want to skip between them.
The \textbf{titles\_to\_skip} and \textbf{dimensions\_to\_skip} work in the same manner for titles and dimensions.

available metrics - list of all metrics

To summarize, the metadata section contains the following fields:
\begin{itemize}[parsep=0pt, itemsep=0pt, leftmargin=0.5cm]
    \item \textbf{parser name} - The name of the parser, used for identification, usually consists of the platform name.
    \item \textbf{data format} - The format of the data, used for identification, please always fill in "data_format": {"name":"simple_format"}.
    \item \textbf{kind} - The kind of the parser, used for identification, please always fill in "non_counter.generic".
    \item \textbf{heuristics} - please always fill in "heuristics": {"conds": [], "kind": "and"}
    \item \textbf{platforms} - A list of platforms which can be processed by this parser, user will tell you the platform.
    \item \textbf{metrics to skip} - A list of metrics that should be skipped during processing.
    \item \textbf{available metrics} - A list of metrics that are available for processing.
    \item \textbf{titles to skip} - A list of titles that should be skipped during processing.
    \item \textbf{dimensions to skip} - A list of dimensions that should be skipped during processing.
    \item \textbf{metric aliases} - A dictionary of metric aliases.
    \item \textbf{dimension aliases} - A dictionary of dimension aliases.
\end{itemize}



This is the final structure:
class ParserDefinition(BaseModel):
    parser_name: str
    data_format: DataFormat
    kind: typing.Literal["non_counter.generic"] = "non_counter.generic"
    heuristics: Heuristics 
    areas: typing.List[NonCounterGeneric]
    platforms: typing.List[str] = []
    metrics_to_skip: typing.List[str] = []
    available_metrics: typing.Optional[typing.List[str]]
    on_metric_check_failed: TableException.Action = TableException.Action.SKIP
    titles_to_skip: typing.List[str] = []
    dimensions_to_skip: typing.Dict[str, typing.List[str]] = {}
    metric_aliases: typing.List[typing.Tuple[str, str]] = []
    dimension_aliases: typing.List[typing.Tuple[str, str]] = []


This it the area structure:
class NonCounterGeneric(BaseModel): #Generic Area Definition
    metrics: typing.Optional[MetricSource]
    data_headers: DataHeaders
    dates: typing.Optional[DateSource]
    titles: typing.Optional[TitleSource]
    title_ids: typing.List[TitleIdSource] = []
    dimensions: typing.List[DimensionSource] = []
    organizations: typing.Optional[OrganizationSource]
    